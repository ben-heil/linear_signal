name: "PytorchSupervised"
optimizer_name: "Adam"
loss_name: "CrossEntropyLoss"
model_name: "PytorchLR"
lr: .001
weight_decay: 0
device: "cuda"
seed: 42
epochs: 50
batch_size: 16
experiment_name: "PytorchLR"
experiment_description: "Train a supervised pytorch model"
log_progress: True
train_fraction: .8
save_path: "logs/models/pytorch_lr.pt"
clip_grads: True
final_layer_name: "fc1"

# config.input_size and config.output_size should be set dynamically
